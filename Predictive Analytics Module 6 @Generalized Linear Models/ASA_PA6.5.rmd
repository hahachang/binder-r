---
title: "Predictive Analtyics Exam Module 6 Section 5"
output: html_notebook
---
Run CHUNK 1 to load the AutoCLaim data set and examine the two target variables.

```{r}
# CHUNK 1
AutoClaim <- read.csv("AutoClaim.csv")
AutoClaim$CLM_FLAG_NUM <- ifelse(AutoClaim$CLM_FLAG == 'Yes', 1, 0)
summary(AutoClaim$CLM_FLAG_NUM)
summary(AutoClaim$CLM_AMT5)
```

CHUNK 2 sets up initial GLMs for the two variables. 

```{r Answer - Initial GLMs}
#CHUNK 2

glm.freq <- glm(formula = CLM_FLAG_NUM ~ AGE + GENDER + MARRIED + JOBCLASS + MAX_EDUC + BLUEBOOK,
                 data = AutoClaim,
                 family = binomial(link="logit")) 

glm.sev <- glm(formula = CLM_AMT5 ~ AGE + GENDER + MARRIED + CAR_USE + BLUEBOOK + CAR_TYPE + AREA,
               # Keeping only observations with positive claim amounts
                 data = AutoClaim[which(AutoClaim$CLM_AMT5 > 0), ],
                 family = Gamma(link='log')) #zzz = Gamma or inverse.gaussian, Select one to be your model.
```

Run CHUNK 3 to see the summary for the fitted frequency model.

```{r}
#CHUNK 3
summary(glm.freq)
```

Run CHUNK 4 to see the exponentiated coefficients. These approximate the multiplicative effect of a unit change in the variable.
  
```{r}
#CHUNK 4
exp(coef(glm.freq))
```
  
Run CHUNK 5 to obtain confidence intervals for the exponentiated coefficients.

```{r}
#CHUNK 5
exp(confint.default(glm.freq))
```

Run CHUNK 6 to examine a reduced model.

```{r}
#CHUNK 6
# Removed MAX_EDUC
glm.freq2 <- glm(formula = CLM_FLAG_NUM ~ AGE + GENDER + MARRIED + JOBCLASS + BLUEBOOK,
                 data = AutoClaim,
                 family = binomial(link='logit')) 

# anova(object, ..., dispersion = NULL, test = NULL)
# test - "Chisq", "LRT", "Rao", "F" or "Cp"
anova(glm.freq, glm.freq2, test = NULL)
anova(glm.freq, glm.freq2, test = "LRT")
```

Run CHUNK 7 to use AIC.

```{r}
#CHUNK 7
AIC(glm.freq, glm.freq2)
drop1(glm.freq) #AIC is the default
```

```{r}
#exercise 6.5.2
drop1(glm.sev) #AIC is the default

# glm.sev <- glm(formula = CLM_AMT5 ~ AGE + GENDER + MARRIED + CAR_USE + BLUEBOOK + CAR_TYPE + AREA,
#                # Keeping only observations with positive claim amounts
#                  data = AutoClaim[which(AutoClaim$CLM_AMT5 > 0), ],
#                family = Gamma(link='log')) #zzz = Gamma or inverse.gaussian, Select one to be your model

glm.sev2 <- glm(formula = CLM_AMT5 ~ GENDER + MARRIED + CAR_USE + BLUEBOOK + CAR_TYPE + AREA,
               # Keeping only observations with positive claim amounts
                 data = AutoClaim[which(AutoClaim$CLM_AMT5 > 0), ],
                 family = Gamma(link='log')) #zzz = Gamma or inverse.gaussian, Select one to be your model
drop1(glm.sev2)

glm.sev3 <- glm(formula = CLM_AMT5 ~ AGE + MARRIED + CAR_USE + BLUEBOOK + CAR_TYPE + AREA,
               # Keeping only observations with positive claim amounts
                 data = AutoClaim[which(AutoClaim$CLM_AMT5 > 0), ],
                 family = Gamma(link='log')) #zzz = Gamma or inverse.gaussian, Select one to be your model
drop1(glm.sev3)

glm.sev4 <- glm(formula = CLM_AMT5 ~ CAR_USE + BLUEBOOK + CAR_TYPE + AREA,
               # Keeping only observations with positive claim amounts
                 data = AutoClaim[which(AutoClaim$CLM_AMT5 > 0), ],
                 family = Gamma(link='log')) #zzz = Gamma or inverse.gaussian, Select one to be your model
drop1(glm.sev4)
```

Run CHUNK 8 to see some residual plots. 

```{r}
#CHUNK 8
library(ggplot2)
library(gridExtra)
p1 <- qplot(x = glm.sev$fitted.values, y = residuals(glm.sev))
p2 <- qplot(x = glm.freq$fitted.values, y = residuals(glm.freq))
grid.arrange(p1, p2, ncol = 2)

```

Chunks 9 and 10 provide function to make "crunch residuals" and then display them for the current frequency model.

```{r}
#CHUNK 9
# Mean residual (actual - predicted)
#----------------------------------------
#you wiil not be responsible for crunch residuals at your exam
#----------------------------------------
library(data.table)
crunch_residual <- function(dataset, model, target, size = 50) {
  x  <- dataset[, target]
  x1 <- model$fitted.values
  y <- as.data.frame(cbind(x, x1))
  
  y <- y[order(x1), ]
  test <- setDT(y)[, as.list(colMeans(.SD)), by = gl(ceiling(nrow(y)/size), size, nrow(y))]
  test$res <- test$x - test$x1

  qplot(y = test$res, x = test$x1, ylab = "Residual", xlab = "Fitted Value", main = "Crunch Residual")
}

crunch_residual(dataset = AutoClaim, 
                model = glm.freq,
                target = 'CLM_FLAG_NUM', 
                size = 50)
```

```{r}
#CHUNK 10
# Deviance
crunch_residual2 <- function(dataset, model, target, size = 50) {
  r  <- model$residuals
  x1 <- model$fitted.values
  y <- as.data.frame(cbind(r, x1))
  
  y <- y[order(x1), ]
  test <- setDT(y)[, as.list(colMeans(.SD)), by = gl(ceiling(nrow(y)/size), size, nrow(y))]

  qplot(y = test$r, x = test$x1, ylab = "Residual", xlab = "Fitted Value", main = "Crunch Residual")
}

crunch_residual2(dataset = AutoClaim, 
                model = glm.freq,
                target = 'CLM_FLAG_NUM', 
                size = 50)
```

CHUNK 11 provides some additional diagnostic plots.

```{r}
#CHUNK 11
#glm.diag.plots(glm.sev, glmdiag=glm.diag(glm.sev))
par(mfrow = c(2, 2))
plot(glm.sev)
```

Begin the example by running CHUNK 12 to create and plot a simple dataset.

```{r echo = FALSE}
#CHUNK 12
library(ggplot2)
df <- data.frame(x = seq(-1, 1, 0.3), y = c(2,1.1,0.7,0.95,0.4,-0.1, -0.05))

ggplot(data = df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3)
```

Run CHUNK 13 to add the desired features to the dataframe.

```{r}
#CHUNK 13
df$X1 = df$x
df$X2 = df$x^2
df$X3 = df$x^3
df$X4 = df$x^4
df$X5 = df$x^5
df$X6 = df$x^6

df
```

Run CHUNK 14 to obtain the ordinary least squares fit, using the glmnet package.

```{r}
#CHUNK 14
library(glmnet)

# Place the X values in a matrix (required for the glmnet function)
X <- as.matrix(df[,3:8])

# Set up the formula (model form)
formula.lm <- as.formula("y~X1+X2+X3+X4+X5+X6")

# Fit the model, lambda = 0 forces ordinary least squares and makes alpha irrelevant
model.lm <- glmnet(X, 
                   y = df$y, 
                   family = "gaussian", 
                   alpha = 0, 
                   lambda = 0)

# Predict results (so we can plot the line)
df$pred <- predict(model.lm, newx = X)

# Plot the results
p1 <- ggplot(data = df, aes(x = x, y = y)) + 
  geom_point(color = "blue", size = 3) + 
  geom_line(aes(y=df$pred))

p1
```

Run CHUNK 15 to view the estimated coefficients.

```{r}
#CHUNK 15
model.lm$beta
sum(model.lm$beta^2)
```

Run CHUNK 16 to perform ridge regression.

```{r}
#CHUNK 16
library(glmnet)

# Here is a clever way to create the data matrix. This one doesn't require keeping track of which columns contain the features.
X <- model.matrix(formula.lm, data = df)

# Lambda has arbitrarily been set to 0.1. Alpha = 0 implies ridge regression (1 implies lasso and anything between is elasticnet). Also, note that the default is to standardize the features, but the estimated coefficients are on the scale and location of the original values.
model.lm.ridge <- glmnet(X, 
                         y = df$y,
                         family = "gaussian",
                         alpha = 0,
                         lambda = 0.1)

# Predict results (so we can plot the line)
df$pred_ridge01 <- predict(model.lm.ridge, newx = X)

# Plot the results
p1 <- ggplot(data = df, aes(x = x, y = y)) + 
  geom_point(color = "blue", size = 3) + 
  geom_line(aes(y=df$pred_ridge01))

p1
```

Run CHUNK 17 to see the coefficients.

```{r}
#CHUNK 17
model.lm.ridge$beta
sum(model.lm.ridge$beta^2)
```

What happens when we change the value for lambda? Run CHUNK 18 to find out.

```{r}
#CHUNK 18
library(gridExtra)
model.lm.ridge00 <- glmnet(X, y = df$y,family = "gaussian", alpha = 0, lambda = 0)
model.lm.ridge01 <- glmnet(X, y = df$y,family = "gaussian", alpha = 0, lambda = 0.1)
model.lm.ridge05 <- glmnet(X, y = df$y,family = "gaussian", alpha = 0, lambda = 0.5)
model.lm.ridge1 <- glmnet(X, y = df$y,family = "gaussian", alpha = 0, lambda = 1)
model.lm.ridge10 <- glmnet(X, y = df$y,family = "gaussian", alpha = 0, lambda = 10)
model.lm.ridge100 <- glmnet(X, y = df$y,family = "gaussian", alpha = 0, lambda = 100)

df$pred_ridge00 <- predict(model.lm.ridge00, newx = X)
df$pred_ridge01 <- predict(model.lm.ridge01, newx = X)
df$pred_ridge05 <- predict(model.lm.ridge05, newx = X)
df$pred_ridge1 <- predict(model.lm.ridge1, newx = X)
df$pred_ridge10 <- predict(model.lm.ridge10, newx = X)
df$pred_ridge100 <- predict(model.lm.ridge100, newx = X)

p1 <- ggplot(data = df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3) + geom_line(aes(y=df$pred_ridge00)) + ggtitle("lambda = 0")
p2 <- ggplot(data = df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3) + geom_line(aes(y=df$pred_ridge01)) + ggtitle("lambda = 0.1")
p3 <- ggplot(data = df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3) + geom_line(aes(y=df$pred_ridge05)) + ggtitle("lambda = 0.5")
p4 <- ggplot(data = df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3) + geom_line(aes(y=df$pred_ridge1)) + ggtitle("lambda = 1")
p5 <- ggplot(data = df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3) + geom_line(aes(y=df$pred_ridge10)) + ggtitle("lambda = 10")
p6 <- ggplot(data = df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3) + geom_line(aes(y=df$pred_ridge100)) + ggtitle("lambda = 100")

grid.arrange(p1,p2,ncol=2)
grid.arrange(p3,p4,ncol=2)
grid.arrange(p5,p6,ncol=2)


```

And the coefficients are produced in CHUNK 19.

```{r}
#CHUNK 19
model.lm.ridge00$beta
sum(model.lm.ridge00$beta^2)
model.lm.ridge01$beta
sum(model.lm.ridge01$beta^2)
model.lm.ridge05$beta
sum(model.lm.ridge05$beta^2)
model.lm.ridge1$beta
sum(model.lm.ridge1$beta^2)
model.lm.ridge10$beta
sum(model.lm.ridge10$beta^2)
model.lm.ridge100$beta
sum(model.lm.ridge100$beta^2)

```

CHUNK 20 is used to make an interesting graph, to explain the difference between ridge and lasso.

```{r echo = FALSE}
#CHUNK 20
x <- seq(-1.5,1.5,0.1)
penalties <- data.frame(beta = x,
                     penalty = x^2,
                     abs = abs(x))

ggplot(data = penalties, aes(x = beta)) +
  geom_line(aes(y = penalty), color = "red") +
  geom_line(aes(y = abs), color = "blue") +
  geom_line(aes(y = 1)) +
  annotate("rect", xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = 1, fill = "blue", alpha = 0.2) + 
  annotate("rect", xmin = -Inf, xmax = Inf, ymin = 1, ymax = Inf, fill = "red", alpha = 0.2) +
  annotate("text", x = -1.4, y = 1.2, label = "Absolute value", color = "blue") + 
  annotate("text", x = -1.1, y = 1.7, label = "Square", color = "red") +
  annotate("text", x = 0, y = 1.3, label = "Square penalty > Absolute value penalty", color = "black") +
  annotate("text", x = 0, y = 0.7, label = "Absolute value penalty > Square penalty", color = "black") +
  ggtitle("Absolute value penalty vs Square penalty")
```


CHUNK 21 performs lasso and ridge regressions on the seven-point data set.

```{r}
#CHUNK 21
library(glmnet)

X <- model.matrix(formula.lm, data = df)

model.lm.ridge <- glmnet(X, y = df$y,family = "gaussian", alpha = 0, lambda = 0.1)
model.lm.lasso <- glmnet(X, y = df$y,family = "gaussian", alpha = 1, lambda = 0.1)

# Predict results (so we can plot the line)
df$pred_ridge01 <- predict(model.lm.ridge, newx = X)
df$pred_lasso01 <- predict(model.lm.lasso, newx = X)

# Plot the results
p1 <- ggplot(data = df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3) + geom_line(aes(y=df$pred_ridge01)) + ggtitle("Ridge - lambda = 0.1")
p2 <- ggplot(data = df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3) + geom_line(aes(y=df$pred_lasso01)) + ggtitle("Lasso - lambda = 0.1")

grid.arrange(p1,p2,ncol=2)
```

CHUNK 22 provides the coefficients.

```{r}
#CHUNK 22
model.lm.ridge$beta
sum(model.lm.ridge$beta^2)
model.lm.lasso$beta
sum(model.lm.lasso$beta^2)
```

CHUNK 23 makes a picture that illustrates the various penalty funcitons.

```{r echo = FALSE}
#CHUNK 23
x <- seq(-1.5,1.5,0.1)
penalties <- data.frame(beta = x,
                     penalty = x^2,
                     abs = abs(x),
                     el025 = 0.25*abs(x)+0.75*x^2,
                     el075 = 0.75*abs(x)+0.25*x^2)

ggplot(data = penalties, aes(x = beta)) +
  geom_line(aes(y = penalty), color = "red") +
  geom_line(aes(y = abs), color = "blue") +
  geom_line(aes(y = el025), color = "green") +
  geom_line(aes(y = el075), color = "purple") +
  geom_line(aes(y = 1)) +
  annotate("rect", xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = 1, fill = "blue", alpha = 0.2) + 
  annotate("rect", xmin = -Inf, xmax = Inf, ymin = 1, ymax = Inf, fill = "red", alpha = 0.2) +
  annotate("text", x = -1.4, y = 1.2, label = "Absolute value", color = "blue") + 
  annotate("text", x = -1.1, y = 1.7, label = "Square", color = "red") +
  annotate("text", x = 1.1, y = 1.7, label = "EN 0.25", color = "green") +
  annotate("text", x = 1.4, y = 1.2, label = "EN 0.75", color = "purple") +
  ggtitle("Absolute value penalty vs Square penalty vs Elastic Net")
```

CHUNK 24 runs elastic net regularization on the sample data.

```{r}
X <- model.matrix(formula.lm, data = df)
#CHUNK 24

model.lm.ridge <- glmnet(X, y = df$y,family = "gaussian", alpha = 0, lambda = 0.1)
model.lm.lasso <- glmnet(X, y = df$y,family = "gaussian", alpha = 1, lambda = 0.1)
model.lm.en025 <- glmnet(X, y = df$y,family = "gaussian", alpha = 0.25, lambda = 0.1)
model.lm.en075 <- glmnet(X, y = df$y,family = "gaussian", alpha = 0.75, lambda = 0.1)

# Predict results (so we can plot the line)
df$pred_ridge01 <- predict(model.lm.ridge, newx = X)
df$pred_lasso01 <- predict(model.lm.lasso, newx = X)
df$pred_en01025 <- predict(model.lm.en025, newx = X)
df$pred_en01075 <- predict(model.lm.en075, newx = X)

# Plot the results
p1 <- ggplot(data = df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3) + geom_line(aes(y=df$pred_ridge01)) + ggtitle("Ridge - lambda = 0.1")
p2 <- ggplot(data = df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3) + geom_line(aes(y=df$pred_lasso01)) + ggtitle("Lasso - lambda = 0.1")
p3 <- ggplot(data = df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3) + geom_line(aes(y=df$pred_en01025)) + ggtitle("Elastic Net-lambda=0.1, alpha=0.25")
p4 <- ggplot(data = df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3) + geom_line(aes(y=df$pred_en01075)) + ggtitle("Elastic Net-lambda=0.1, alpha=0.75")

grid.arrange(p1,p2,ncol=2)
grid.arrange(p3,p4,ncol=2)
```

CHUNK 25 displays the coefficients

```{r}
#CHUNK 25
model.lm$beta
model.lm.ridge$beta
model.lm.lasso$beta
model.lm.en025$beta
model.lm.en075$beta
```


